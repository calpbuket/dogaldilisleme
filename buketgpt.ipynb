{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import mmap\n",
    "import random \n",
    "\n",
    "block_size = 64\n",
    "batch_size = 32\n",
    "max_iters = 1500\n",
    "learning_rate = 5e-5\n",
    "eval_iters = 100\n",
    "n_embd = 384\n",
    "n_layer = 1\n",
    "n_head = 1\n",
    "dropout = 0.2\n",
    "tf.random.set_seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/buketcalp/vocab.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: \"\".join([int_to_string[i] for i in l])\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"/Users/buketcalp/output_train.txt\" if split == \"train\" else \"/Users/buketcalp/output_val.txt\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size)\n",
    "            decode_block = block.decode(\"utf-8\", errors=\"ignore\").replace(\"\\r\", \"\")\n",
    "            data = np.array([string_to_int[c] for c in decode_block], dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "def get_batch(split, batch_size=batch_size):\n",
    "    data = get_random_chunk(split)\n",
    "    min_index = max(len(data) - block_size, 0)\n",
    "    ix = np.random.randint(min_index, size=(batch_size,)) if min_index > 0 else np.random.randint(1, size=(batch_size,))\n",
    "    x = np.stack([data[i:i + block_size] for i in ix])\n",
    "    y = np.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    sequence_length = min(x.shape[1], y.shape[1])\n",
    "    x = x[:, :sequence_length]\n",
    "    y = y[:, :sequence_length]\n",
    "    return x, y\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.query_dense = layers.Dense(num_heads * head_size)\n",
    "        self.key_dense = layers.Dense(num_heads * head_size)\n",
    "        self.value_dense = layers.Dense(num_heads * head_size)\n",
    "        self.dense = layers.Dense(n_embd)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.tril = np.tril(np.ones((block_size, block_size)))\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_size))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "        mask = tf.convert_to_tensor(self.tril, dtype=tf.float32)\n",
    "        mask = mask[:tf.shape(scaled_attention_logits)[-2], :tf.shape(scaled_attention_logits)[-1]]\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        scaled_attention = tf.matmul(attention_weights, value)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.head_size))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_embd):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            layers.Dense(4 * n_embd, activation='relu'),\n",
    "            layers.Dense(n_embd),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.seq(inputs)\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super(Block, self).__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.mha = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.mha(self.layernorm1(inputs))\n",
    "        out1 = inputs + attn_output\n",
    "        ffn_output = self.ffn(self.layernorm2(out1))\n",
    "        return out1 + ffn_output\n",
    "\n",
    "class GPTLanguageModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(GPTLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = layers.Embedding(block_size, n_embd)\n",
    "        self.blocks = [Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        self.ln_f = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.lm_head = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, index, targets=None):\n",
    "        B = tf.shape(index)[0]\n",
    "        T = tf.shape(index)[1]\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(tf.range(T))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits_flat = tf.reshape(logits, [-1, tf.shape(logits)[-1]])\n",
    "            targets_flat = tf.reshape(targets, [-1])\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(targets_flat, logits_flat, from_logits=True)\n",
    "            return logits, tf.reduce_mean(loss)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            logits = self(index_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = tf.nn.softmax(logits, axis=-1)\n",
    "            index_next = tf.random.categorical(probs, num_samples=1)\n",
    "            index = tf.concat([index, index_next], axis=1)\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 10.439, val loss: 10.443\n",
      "step: 100, train loss: 6.380, val loss: 6.383\n",
      "step: 200, train loss: 3.277, val loss: 3.288\n",
      "step: 300, train loss: 1.777, val loss: 1.792\n",
      "step: 400, train loss: 0.960, val loss: 0.976\n",
      "step: 500, train loss: 0.650, val loss: 0.695\n",
      "step: 600, train loss: 0.481, val loss: 0.588\n",
      "step: 700, train loss: 0.406, val loss: 0.382\n",
      "step: 800, train loss: 0.309, val loss: 0.322\n",
      "step: 900, train loss: 0.241, val loss: 0.240\n",
      "step: 1000, train loss: 0.212, val loss: 0.200\n",
      "step: 1100, train loss: 0.305, val loss: 0.215\n",
      "step: 1200, train loss: 0.275, val loss: 0.154\n",
      "step: 1300, train loss: 0.147, val loss: 0.156\n",
      "step: 1400, train loss: 0.136, val loss: 0.139\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(inputs, targets)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def eval_step(inputs, targets):\n",
    "    logits, loss = model(inputs, targets)\n",
    "    return loss\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        train_losses = [eval_step(*get_batch('train')) for _ in range(eval_iters)]\n",
    "        val_losses = [eval_step(*get_batch('val')) for _ in range(eval_iters)]\n",
    "        print(f\"step: {iter}, train loss: {np.mean(train_losses):.3f}, val loss: {np.mean(val_losses):.3f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    train_step(xb, yb)\n",
    "\n",
    "\n",
    "model.save_weights('model-01.h5')\n",
    "print('Model saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
